# HyperFleet Infrastructure - Terraform

Terraform configuration for creating personal HyperFleet development clusters.

## Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                     hcm-hyperfleet project                      │
│                                                                 │
│  ┌───────────────────────────────────────────────────────────┐  │
│  │              hyperfleet-dev-vpc (shared)                  │  │
│  │                                                           │  │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐       │  │
│  │  │ hyperfleet- │  │ hyperfleet- │  │ hyperfleet- │       │  │
│  │  │ dev-alice   │  │ dev-bob     │  │ dev-carol   │  ...  │  │
│  │  │ (GKE)       │  │ (GKE)       │  │ (GKE)       │       │  │
│  │  └─────────────┘  └─────────────┘  └─────────────┘       │  │
│  │                                                           │  │
│  └───────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────┘
```

- **Shared VPC**: One VPC for all dev clusters (deployed once per project)
- **Per-developer clusters**: Each developer gets their own isolated GKE cluster

## Prerequisites

- [Terraform](https://developer.hashicorp.com/terraform/downloads) >= 1.5
- [Google Cloud SDK](https://cloud.google.com/sdk/docs/install) (`gcloud`)
- [gke-gcloud-auth-plugin](https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl#install_plugin) (for kubectl access)
- `kubectl`
- Access to the `hcm-hyperfleet` GCP project
- Shared infrastructure deployed (see [Shared Infrastructure](#shared-infrastructure) below)

### Installing gke-gcloud-auth-plugin

```bash
# If gcloud was installed via package manager (dnf/apt):
sudo dnf install google-cloud-sdk-gke-gcloud-auth-plugin  # Fedora/RHEL
# OR
sudo apt-get install google-cloud-sdk-gke-gcloud-auth-plugin  # Debian/Ubuntu

# If gcloud was installed via gcloud installer:
gcloud components install gke-gcloud-auth-plugin
```

## Quick Start (For Developers)

> **Note**: The shared VPC must be deployed first. See [Shared Infrastructure](#shared-infrastructure) below.

```bash
# 1. Authenticate with GCP
gcloud auth application-default login
gcloud config set project hcm-hyperfleet

# 2. Initialize Terraform
cd terraform
terraform init

# 3. Create your tfvars file
cp envs/gke/dev.tfvars.example envs/gke/dev-<username>.tfvars

# 4. Edit your tfvars - set developer_name to your username
#    e.g., developer_name = "your-username"

# 5. Plan (review what will be created)
terraform plan -var-file=envs/gke/dev-<username>.tfvars

# 6. Apply (create the cluster)
terraform apply -var-file=envs/gke/dev-<username>.tfvars

# 7. Connect to your cluster (command shown in terraform output)
gcloud container clusters get-credentials hyperfleet-dev-<username> \
  --zone us-central1-a \
  --project hcm-hyperfleet

# 8. Verify
kubectl get nodes
```

### Using Shared Configuration

For shared environment configuration, use `dev-shared.tfvars` in addition to your personal tfvars:

```bash
# Apply with both shared and personal configuration
terraform apply \
  -var-file=envs/gke/dev-shared.tfvars \
  -var-file=envs/gke/dev-<username>.tfvars
```

Personal tfvars override shared values, so you can customize specific settings while inheriting common defaults.

## Destroying Your Cluster

**Always destroy your cluster when you're done to avoid unnecessary costs.**

```bash
terraform destroy -var-file=envs/gke/dev-<username>.tfvars
```

## Configuration Options

| Variable | Description | Default |
|----------|-------------|---------|
| `developer_name` | Your username (used in cluster name) | **required** |
| `cloud_provider` | Cloud provider (`gke`, `eks`, `aks`) | `gke` |
| `gcp_project_id` | GCP project | `hcm-hyperfleet` |
| `gcp_zone` | GCP zone | `us-central1-a` |
| `gcp_network` | VPC network name | `hyperfleet-dev-vpc` |
| `gcp_subnetwork` | Subnet name | `hyperfleet-dev-vpc-subnet` |
| `node_count` | Number of nodes | `1` |
| `machine_type` | VM instance type | `e2-standard-4` |
| `use_spot_vms` | Use Spot VMs for cost savings | `true` |
| `kubernetes_namespace` | Kubernetes namespace for Workload Identity binding | `hyperfleet-system` |
| `use_pubsub` | Use Google Pub/Sub for messaging (instead of RabbitMQ) | `false` |
| `enable_dead_letter` | Enable dead letter queue for Pub/Sub | `true` |
| `pubsub_topic_configs` | Map of Pub/Sub topic configurations with adapter subscriptions | See below |

## Cost Optimization

- **Spot VMs** are enabled by default (~70% cost savings)
- Spot VMs may be preempted with 30 seconds notice
- For stable workloads, set `use_spot_vms = false`
- **Always destroy when done** - clusters cost ~$3-5/day with Spot VMs

## Google Pub/Sub (Optional)

Enable Pub/Sub to use Google's managed message broker instead of RabbitMQ.

### Enable Pub/Sub

Add to your tfvars file:

```hcl
kubernetes_namespace = "hyperfleet-system"  # Kubernetes namespace for Workload Identity binding
use_pubsub = true
enable_dead_letter = true  # Optional, defaults to true

# Configure topics and their adapter subscriptions
pubsub_topic_configs = {
  clusters = {
    adapter_subscriptions = {
      landing-zone   = {}
      validation-gcp = {}
    }
  }
  nodepools = {
    adapter_subscriptions = {
      validation-gcp = {}
    }
  }
}
```

Or pass as command line arguments:

```bash
terraform apply -var-file=envs/gke/dev-<username>.tfvars \
  -var="use_pubsub=true"
```

### Customizing Topics and Subscriptions

Each topic can have its own set of adapter subscriptions. Per-subscription settings can be configured:

```hcl
pubsub_topic_configs = {
  clusters = {
    message_retention_duration = "604800s"  # 7 days (optional)
    adapter_subscriptions = {
      landing-zone = {
        ack_deadline_seconds = 60  # Default: 60
      }
      validation-gcp = {
        ack_deadline_seconds = 120  # Custom setting for this adapter
      }
    }
  }
  nodepools = {
    adapter_subscriptions = {
      validation-gcp = {}  # Only validation-gcp subscribes to nodepools
    }
  }
  volumes = {
    adapter_subscriptions = {
      landing-zone = {}
      orchestrator = {}
    }
  }
}
```

When you add or remove topics/subscriptions and re-run `terraform apply`, the infrastructure will be updated accordingly.

> **Note**: A single sentinel instance can only watch one resource type (clusters OR nodepools) and publish to one topic. If you need to watch multiple resource types:
>
> 1. Deploy separate sentinel instances (e.g., `sentinel-clusters`, `sentinel-nodepools`)
> 2. Each instance uses the same GCP service account (`sentinel-{developer}`) which has publish permissions on all topics
> 3. Configure each sentinel with its topic from `pubsub_config.sentinel.topics.<resource_type>`
>
> The `pubsub_config` output provides complete configuration data for all topics, subscriptions, and service accounts.

### What It Creates

| Resource | Name Pattern | Description |
|----------|--------------|-------------|
| Pub/Sub Topics | `{kubernetes_namespace}-{topic_name}-{developer}` | Event topics (clusters, nodepools, etc.) |
| Pub/Sub Subscriptions | `{kubernetes_namespace}-{topic_name}-{adapter}-adapter-{developer}` | Subscriptions per topic per adapter |
| Dead Letter Topics | `{kubernetes_namespace}-{topic_name}-{developer}-dlq` | Failed message storage (optional) |
| Service Accounts | `sentinel-{developer}` | Single publisher SA for all topics (one sentinel publishes to all topics) |
| Service Accounts | `{adapter}-{developer}` | Subscriber SA per unique adapter |
| Workload Identity | - | Binds K8s SAs to GCP SAs |

**Example with developer `alice` and config:**
```hcl
pubsub_topic_configs = {
  clusters  = { adapter_subscriptions = { landing-zone = {}, validation-gcp = {} } }
  nodepools = { adapter_subscriptions = { validation-gcp = {} } }
}
```

**Creates:**
- **Topics:**
  - `hyperfleet-system-clusters-alice`
  - `hyperfleet-system-nodepools-alice`
- **Subscriptions:**
  - `hyperfleet-system-clusters-landing-zone-adapter-alice`
  - `hyperfleet-system-clusters-validation-gcp-adapter-alice`
  - `hyperfleet-system-nodepools-validation-gcp-adapter-alice`
- **Service Accounts:**
  - `sentinel-alice` (publishes to all topics: clusters, nodepools)
  - `landing-zone-alice` (subscribes to clusters only)
  - `validation-gcp-alice` (subscribes to both topics)

Each developer gets completely isolated Pub/Sub resources - no conflicts between developer environments.

### IAM Permissions

The module configures resource-level IAM permissions following the principle of least privilege:

**Sentinel Service Account** (`sentinel-{developer}`):
- `roles/pubsub.publisher` on **all topics** - Publish messages to any topic
- `roles/pubsub.viewer` on **all topics** - View topic metadata (required to check if topic exists)
- `roles/iam.workloadIdentityUser` on **service account** - Allow K8s SA `sentinel` to impersonate this GCP SA

**Adapter Service Accounts** (`{adapter}-{developer}`):
- `roles/pubsub.subscriber` on **their subscriptions** - Pull and acknowledge messages from subscriptions across all topics
- `roles/pubsub.viewer` on **their subscriptions** - View subscription metadata
- `roles/iam.workloadIdentityUser` on **service account** - Allow K8s SA `{adapter}-adapter` to impersonate this GCP SA

**Note**: These are resource-level IAM bindings, not project-level roles. There is one shared sentinel GCP service account with publish access to all topics. Each adapter has one GCP service account that can access their subscriptions across all topics, but adapters cannot access topics directly or other adapters' subscriptions.

### Outputs

After applying with `use_pubsub=true`, you'll get the `pubsub_config` output containing all Pub/Sub configuration data:

```bash
# Get complete Pub/Sub configuration
terraform output -json pubsub_config
```

**Output structure:**

```json
{
  "project_id": "hcm-hyperfleet",
  "sentinel": {
    "service_account_email": "sentinel-alice@hcm-hyperfleet.iam.gserviceaccount.com",
    "k8s_service_account": "sentinel",
    "topics": {
      "clusters": {
        "topic_name": "hyperfleet-system-clusters-alice",
        "topic_id": "projects/hcm-hyperfleet/topics/hyperfleet-system-clusters-alice",
        "dlq_topic_name": "hyperfleet-system-clusters-alice-dlq",
        "resource_type": "clusters"
      },
      "nodepools": {
        "topic_name": "hyperfleet-system-nodepools-alice",
        "topic_id": "projects/hcm-hyperfleet/topics/hyperfleet-system-nodepools-alice",
        "dlq_topic_name": "hyperfleet-system-nodepools-alice-dlq",
        "resource_type": "nodepools"
      }
    }
  },
  "adapters": {
    "landing-zone": {
      "service_account_email": "landing-zone-alice@hcm-hyperfleet.iam.gserviceaccount.com",
      "subscriptions": {
        "clusters": {
          "topic_name": "hyperfleet-system-clusters-alice",
          "subscription_name": "hyperfleet-system-clusters-landing-zone-adapter-alice",
          "dlq_topic_name": "hyperfleet-system-clusters-alice-dlq",
          "ack_deadline": 60
        }
      }
    },
    "validation-gcp": {
      "service_account_email": "validation-gcp-alice@hcm-hyperfleet.iam.gserviceaccount.com",
      "subscriptions": {
        "clusters": {
          "topic_name": "hyperfleet-system-clusters-alice",
          "subscription_name": "hyperfleet-system-clusters-validation-gcp-adapter-alice",
          "dlq_topic_name": "hyperfleet-system-clusters-alice-dlq",
          "ack_deadline": 60
        },
        "nodepools": {
          "topic_name": "hyperfleet-system-nodepools-alice",
          "subscription_name": "hyperfleet-system-nodepools-validation-gcp-adapter-alice",
          "dlq_topic_name": "hyperfleet-system-nodepools-alice-dlq",
          "ack_deadline": 60
        }
      }
    }
  },
  "topics": {
    "clusters": {
      "topic_name": "hyperfleet-system-clusters-alice",
      "topic_id": "projects/hcm-hyperfleet/topics/hyperfleet-system-clusters-alice",
      "dlq_topic_name": "hyperfleet-system-clusters-alice-dlq"
    },
    "nodepools": {
      "topic_name": "hyperfleet-system-nodepools-alice",
      "topic_id": "projects/hcm-hyperfleet/topics/hyperfleet-system-nodepools-alice",
      "dlq_topic_name": "hyperfleet-system-nodepools-alice-dlq"
    }
  }
}
```

**Extracting specific values with jq:**

```bash
# Get sentinel service account for Workload Identity
terraform output -json pubsub_config | jq -r '.sentinel.service_account_email'

# Get topic name for a specific resource type
terraform output -json pubsub_config | jq -r '.sentinel.topics.clusters.topic_name'

# Get adapter subscription details
terraform output -json pubsub_config | jq -r '.adapters["validation-gcp"].subscriptions.clusters'

# List all adapter names
terraform output -json pubsub_config | jq -r '.adapters | keys[]'
```

### Helm Configuration

Use the `pubsub_config` output to construct your Helm values. See [examples/gcp-pubsub/values.yaml](https://github.com/openshift-hyperfleet/hyperfleet-chart/blob/main/examples/gcp-pubsub/values.yaml) in the hyperfleet-chart repository for a complete example.

**Chart Structure Note**: The hyperfleet-gcp chart uses a base + overlay pattern:
- `base:` - Core platform components (API, Sentinel, Landing Zone, RabbitMQ)
- Root level - GCP-specific components (validation-gcp)

See the [hyperfleet-chart README](https://github.com/openshift-hyperfleet/hyperfleet-chart) for full documentation.

## Directory Structure

```
terraform/
├── main.tf                 # Root module (developer clusters)
├── variables.tf            # Input variables
├── outputs.tf              # Cluster outputs
├── providers.tf            # Provider configuration
├── versions.tf             # Version constraints
├── shared/                 # Shared infrastructure (deploy once)
│   ├── main.tf             # VPC, subnet, firewall, NAT
│   ├── variables.tf
│   ├── outputs.tf
│   └── README.md
├── modules/
│   ├── cluster/
│   │   └── gke/            # GKE cluster module
│   └── pubsub/             # Google Pub/Sub module
└── envs/
    └── gke/
        └── dev.tfvars.example
```

## Shared Infrastructure

The `shared/` directory contains Terraform for the VPC and networking that developer clusters use.

**This only needs to be deployed once per GCP project.**

### What It Creates

| Resource | Name | Description |
|----------|------|-------------|
| VPC | `hyperfleet-dev-vpc` | Virtual network for all dev clusters |
| Subnet | `hyperfleet-dev-vpc-subnet` | 10.100.0.0/16 for node IPs |
| Secondary Range | `pods` | 10.101.0.0/16 for pod IPs |
| Secondary Range | `services` | 10.102.0.0/16 for service IPs |
| Firewall | `allow-internal` | Allow traffic within VPC |
| Firewall | `allow-iap-ssh` | Allow SSH via IAP |
| Cloud NAT | `hyperfleet-dev-vpc-nat` | Internet access for private nodes |

### Deploy Shared Infrastructure

```bash
cd terraform/shared
terraform init
terraform plan
terraform apply
```

### Destroy Shared Infrastructure

> **Warning**: Only destroy when ALL developer clusters have been destroyed first!

```bash
cd terraform/shared
terraform destroy
```

See [shared/README.md](shared/README.md) for more details.

## Multi-Cloud Support

The configuration is designed to support multiple cloud providers. Currently only GKE is implemented.

To add EKS or AKS support in the future:
1. Create `modules/cluster/eks/` or `modules/cluster/aks/`
2. Add the module call in `main.tf`
3. Update outputs in `outputs.tf`

## Troubleshooting

### "No network named X" error
The shared VPC hasn't been deployed yet. Deploy it first:
```bash
cd terraform/shared && terraform apply
```

### "Quota exceeded" error
Your GCP project may have hit resource limits. Check quotas in the GCP Console or use a different zone.

### Cluster creation times out
GKE cluster creation typically takes 5-10 minutes. If it takes longer, check the GCP Console for errors.
